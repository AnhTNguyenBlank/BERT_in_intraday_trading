{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a874fc-cde5-421c-9ce5-fa29657960b5",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "390f32dc-525e-4272-9e1c-5259d53cdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('classic')\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062b7a08-bd63-4bb8-a417-fd54fa6f32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, 'D:/BERT_in_intraday_trading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd486a22-ddda-4d28-a304-4bf14e7ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.support import *\n",
    "from src.backtest import *\n",
    "from src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd163c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba65158-e1f2-4f9f-a867-beb1960e6f8a",
   "metadata": {},
   "source": [
    "# Import and pre-processing news data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727780a",
   "metadata": {},
   "source": [
    "## News_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5d8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/BERT_in_intraday_trading/Training/Data/stored_data.pkl\", \"rb\") as f:\n",
    "    news_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f4ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb342adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fake labels\n",
    "\n",
    "for new in news_data:\n",
    "    new['LABEL'] = random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e96df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TITLE': \"'Why stock exchanges never sanctioned Jane Street?': Experts ask after Rs 4,843 crore market manipulation case\",\n",
       " 'URL': 'https://www.businesstoday.in/markets/story/why-stock-exchanges-never-sanctioned-jane-street-experts-question-after-rs-4843-crore-market-manipulation-case-483273-2025-07-06',\n",
       " 'DATE_POSTED': '2025-07-06',\n",
       " 'TIME_POSTED': datetime.datetime(2025, 7, 6, 14, 14, tzinfo=datetime.timezone(datetime.timedelta(seconds=25200))),\n",
       " 'CONTENT': 'As the Securities and Exchange Board of India (SEBI) cracks down on US-based trading firm Jane Street over alleged index manipulation, several investors and market experts have raised sharp questions about the conduct of Indian exchanges and the regulator itself.\\n\\n\"Key question: how come the Stock Exchanges never sanctioned Jane Street?\" investor Shankar Sharma asked in a post on Sunday. \"They are the very first to get such alerts. Will tell you why: simple conflict of interest. I have long held that Exchanges should NEVER get listed. They are a regulator. Profit motive creates endless conflict of interest. How can they sanction JS when it drives FO volume massively, hence SE profits. SEs should be a utility, not for-profits. Simple as that. Nahi to, suffer all this hanky panky.\"\\n\\nFinazenn founder Hemang Jani pointed to what he said was a lack of follow-through even after SEBI\\'s own report flagged media alerts. \"SEBI report says there were media reports flagging off manipulation and then it asked the exchange to investigate,\" he wrote. \"Exchanges & Sebi have the best of surveillance mechanism, but for some reasons it chose to ignore & just sent a warning letter. Why this was allowed to continue for more than 2 years?\"\\n\\nSEBI\\'s interim order has barred four Jane Street entities from accessing Indian securities markets and directed them to disgorge Rs 4,843.57 crore in alleged unlawful gains. The order said that Jane Street entities manipulated index levels on expiry days to gain a significant edge in the index options segment.\\n\\nZerodha CEO Nithin Kamath also reacted to the action: \"You\\'ve got to hand it to SEBI for going after Jane Street. If the allegations are true, it’s blatant market manipulation. The shocking part? They kept at it even after receiving warnings from the exchanges.\"\\n\\nHe added that \"prop trading firms like Jane Street account for nearly 50% of options trading volumes. If they pull back—which seems likely—retail activity (~35%) could take a hit too. So this could be bad news for both exchanges and brokers.\"\\n\\nSEBI\\'s order reveals that Jane Street was cautioned by the NSE in February 2025 after being flagged for manipulative behaviour. Jane Street responded, promising compliance. But by May, the regulator said, they were again engaged in \"egregious behaviour\" aimed at shifting the index in their favour.\\n\\nSEBI whole-time member Ananth Narayan G stated: \"Unlike the vast majority of Foreign Portfolio Investors and other market participants, JS Group is not a good faith actor that can be, or deserves to be, trusted.\"\\n\\nCA Anshul Garg noted that India\\'s high volume of options trading itself needed scrutiny. \"F&O is a legal gamble. Now imagine who to make money from F&O. SEBI, NSE, BSE, Brokers, Big Private players and who are losing—retailers. It is pit digged by its own people.\"',\n",
       " 'LABEL': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc697dd-2579-4d0c-9445-a9a536274807",
   "metadata": {},
   "source": [
    "## Interval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de595335-c8b7-4a00-9323-80b96a840c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_data = pd.read_pickle('D:/Trading_backtest_framework/Training/Data/XAUUSD_M15_1.pkl')\n",
    "\n",
    "interval_data = interval_data.set_index('DATE_TIME')\n",
    "interval_data.index = pd.to_datetime(interval_data.index)\n",
    "\n",
    "interval_data['DATE'] = pd.to_datetime(interval_data['DATE'])\n",
    "interval_data['OPEN'] = interval_data['OPEN']\n",
    "interval_data['HIGH'] = interval_data['HIGH']\n",
    "interval_data['LOW'] = interval_data['LOW']\n",
    "interval_data['CLOSE'] = interval_data['CLOSE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4bd551-c4c4-4d15-b638-0327ec995547",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a86c3b-fa28-48bf-b532-b86425b6644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15_min = prepare_df(df = interval_data, timeframe = '15min', add_indicators = True)\n",
    "\n",
    "df_15_min['WHOLE_RANGE'] = df_15_min['HIGH'] - df_15_min['LOW']\n",
    "df_15_min['GRP_WHOLE_RANGE'] = pd.qcut(df_15_min['WHOLE_RANGE'], 10)\n",
    "df_15_min['GRP_BODY'] = pd.qcut(df_15_min['BODY'], 10)\n",
    "df_15_min['YEAR'] = df_15_min.index.strftime('%Y')\n",
    "df_15_min['WEEK'] = df_15_min.index.strftime('%Y%W')\n",
    "df_15_min['MONTH'] = df_15_min.index.strftime('%Y%m')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40eca3f-3b98-497e-88db-7c974cebd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15_min.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d84add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15_min.index[0], df_15_min.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51674bdf-0908-446c-b5d8-575f9de4b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_df(df_1_day, \n",
    "#         path = None,# 'D:/Intraday_trading/Training/Saved_results/plot_df.html', \n",
    "#         open_tab = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6106353",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.to_datetime(news_data[0]['TIME_POSTED']).tz_localize(None)\n",
    "df_15_min[(df_15_min.index >= timestamp - pd.Timedelta(days = 1)) & (df_15_min.index <= timestamp + pd.Timedelta(days = 1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b39bb4",
   "metadata": {},
   "source": [
    "# Import BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607dbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "# from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f53e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "# dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "#                                   untar=True, cache_dir='.',\n",
    "#                                   cache_subdir='')\n",
    "\n",
    "# dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "# train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# # remove unused folders to make it easier to load the data\n",
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d3eb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [entry['CONTENT'] for entry in news_data]\n",
    "labels = [entry['LABEL'] for entry in news_data]\n",
    "\n",
    "# Split\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Wrap into datasets\n",
    "def prepare_labeled_dataset(texts, labels, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    ds = ds.shuffle(buffer_size=len(texts))\n",
    "    ds = ds.batch(batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = prepare_labeled_dataset(train_texts, train_labels)\n",
    "val_ds = prepare_labeled_dataset(val_texts, val_labels)\n",
    "test_ds = prepare_labeled_dataset(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83d79dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: As Bengaluru’s traffic crisis continues to dominate headlines and social media feeds, a sharply word ...\n",
      "Label  : 0\n",
      "\n",
      "Content: Microsoft has denied speculation that Xbox chief Phil Spencer is planning to retire, following a wee ...\n",
      "Label  : 1\n",
      "\n",
      "Content: Shares of Asian Paints are down 28% from their 52-week high in a less than a year. Of late, the stoc ...\n",
      "Label  : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        text = text_batch.numpy()[i].decode('utf-8')\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Content: {text[:100]} ...')\n",
    "        print(f'Label  : {label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ea1584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af2fcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7755bcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       : ['As the Securities and Exchange Board of India (SEBI) cracks down on US-based trading firm Jane Street over alleged index manipulation, several investors and market experts have raised sharp questions about the conduct of Indian exchanges and the regulator itself.\\n\\n\"Key question: how come the Stock Exchanges never sanctioned Jane Street?\" investor Shankar Sharma asked in a post on Sunday. \"They are the very first to get such alerts. Will tell you why: simple conflict of interest. I have long held that Exchanges should NEVER get listed. They are a regulator. Profit motive creates endless conflict of interest. How can they sanction JS when it drives FO volume massively, hence SE profits. SEs should be a utility, not for-profits. Simple as that. Nahi to, suffer all this hanky panky.\"\\n\\nFinazenn founder Hemang Jani pointed to what he said was a lack of follow-through even after SEBI\\'s own report flagged media alerts. \"SEBI report says there were media reports flagging off manipulation and then it asked the exchange to investigate,\" he wrote. \"Exchanges & Sebi have the best of surveillance mechanism, but for some reasons it chose to ignore & just sent a warning letter. Why this was allowed to continue for more than 2 years?\"\\n\\nSEBI\\'s interim order has barred four Jane Street entities from accessing Indian securities markets and directed them to disgorge Rs 4,843.57 crore in alleged unlawful gains. The order said that Jane Street entities manipulated index levels on expiry days to gain a significant edge in the index options segment.\\n\\nZerodha CEO Nithin Kamath also reacted to the action: \"You\\'ve got to hand it to SEBI for going after Jane Street. If the allegations are true, it’s blatant market manipulation. The shocking part? They kept at it even after receiving warnings from the exchanges.\"\\n\\nHe added that \"prop trading firms like Jane Street account for nearly 50% of options trading volumes. If they pull back—which seems likely—retail activity (~35%) could take a hit too. So this could be bad news for both exchanges and brokers.\"\\n\\nSEBI\\'s order reveals that Jane Street was cautioned by the NSE in February 2025 after being flagged for manipulative behaviour. Jane Street responded, promising compliance. But by May, the regulator said, they were again engaged in \"egregious behaviour\" aimed at shifting the index in their favour.\\n\\nSEBI whole-time member Ananth Narayan G stated: \"Unlike the vast majority of Foreign Portfolio Investors and other market participants, JS Group is not a good faith actor that can be, or deserves to be, trusted.\"\\n\\nCA Anshul Garg noted that India\\'s high volume of options trading itself needed scrutiny. \"F&O is a legal gamble. Now imagine who to make money from F&O. SEBI, NSE, BSE, Brokers, Big Private players and who are losing—retailers. It is pit digged by its own people.\"']\n",
      "Keys       : ['input_mask', 'input_type_ids', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [[  101  2004  1996 12012  1998  3863  2604  1997  2634  1006  7367  5638\n",
      "   1007 15288  2091  2006  2149  1011  2241  6202  3813  4869  2395  2058\n",
      "   6884  5950 16924  1010  2195  9387  1998  3006  8519  2031  2992  4629\n",
      "   3980  2055  1996  6204  1997  2796 15800  1998  1996 21618  2993  1012\n",
      "   1000  3145  3160  1024  2129  2272  1996  4518 15800  2196 14755  4869\n",
      "   2395  1029  1000 14316 17429 14654  2356  1999  1037  2695  2006  4465\n",
      "   1012  1000  2027  2024  1996  2200  2034  2000  2131  2107  9499  2015\n",
      "   1012  2097  2425  2017  2339  1024  3722  4736  1997  3037  1012  1045\n",
      "   2031  2146  2218  2008 15800  2323  2196  2131  3205  1012  2027  2024\n",
      "   1037 21618  1012  5618 15793  9005 10866  4736  1997  3037  1012  2129\n",
      "   2064  2027  2624  7542  1046  2015  2043   102]]\n",
      "Input Mask : [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "Type Ids   : [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text_test = [news_data[0]['CONTENT']]\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Text       : {text_test}')\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf1e7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cba289e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.7592168   0.94622964  0.00941134  0.3245973   0.01596091  0.75140387\n",
      "  0.9950055  -0.5772077  -0.7226336  -0.32523328 -0.0523226  -0.99484664]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[ 0.19264817  0.10760467  0.80577904 ... -1.4146446  -0.6839025\n",
      "   0.19541788]\n",
      " [-0.6332559   0.6054614  -0.83292025 ... -1.1068039  -0.5312409\n",
      "   1.0878513 ]\n",
      " [-0.44336587  1.0099317  -1.2073686  ... -0.9540061  -0.8191695\n",
      "   0.71053034]\n",
      " ...\n",
      " [ 0.06172272  0.21059106 -0.9397543  ... -0.6131694   0.2277433\n",
      "   1.3664987 ]\n",
      " [-0.35575163  0.41886777  0.3598801  ... -0.7755376   1.1650244\n",
      "  -0.14792846]\n",
      " [-0.25871518  0.6753899  -0.3992566  ... -0.15902667  0.4556429\n",
      "   1.3753266 ]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3f04bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df0026c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.89702564]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant([news_data[0]['CONTENT']]))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11c4642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea27a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(\n",
    "#     learning_rate=init_lr,\n",
    "#     weight_decay=0.01\n",
    "# )\n",
    "\n",
    "classifier_model.compile(optimizer = 'adam',\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e41a5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "12/12 [==============================] - 25s 2s/step - loss: 1.0913 - binary_accuracy: 0.4632 - val_loss: 0.6947 - val_binary_accuracy: 0.5679\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "198c9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 461ms/step - loss: 0.6947 - binary_accuracy: 0.5610\n",
      "Loss: 0.694680392742157\n",
      "Accuracy: 0.5609756112098694\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "898cc6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 124). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'news_data'\n",
    "saved_model_path = 'D:/BERT_in_intraday_trading/Training/Saved_results/{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ea53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_model = tf.saved_model.load(saved_model_path)\n",
    "\n",
    "# def print_my_examples(inputs, results):\n",
    "#   result_for_printing = \\\n",
    "#     [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "#                          for i in range(len(inputs))]\n",
    "#   print(*result_for_printing, sep='\\n')\n",
    "#   print()\n",
    "\n",
    "\n",
    "# examples = [\n",
    "#     'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "#     'The movie was great!',\n",
    "#     'The movie was meh.',\n",
    "#     'The movie was okish.',\n",
    "#     'The movie was terrible...'\n",
    "# ]\n",
    "\n",
    "# reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "# original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "# print('Results from the saved model:')\n",
    "# print_my_examples(examples, reloaded_results)\n",
    "# print('Results from the model in memory:')\n",
    "# print_my_examples(examples, original_results)\n",
    "\n",
    "\n",
    "# serving_results = reloaded_model \\\n",
    "#             .signatures['serving_default'](tf.constant(examples))\n",
    "\n",
    "# serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "# print_my_examples(examples, serving_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
